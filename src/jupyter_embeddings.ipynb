{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pmi-embeddings\n",
    "\n",
    "This is a tutorial how to use pmi-embeddings.py. It does not tell you step-by-step how to write your own script for producing word embeddings, but it does guide you through the basic idea and parameters of the tool.\n",
    "\n",
    "For the command line use, you can refer to [this document](https://docs.google.com/document/d/1TjVWqrhalCDjkOQf-JLk1jmC6N83MWGUIEVjbJpm9Es).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import make_embeddings as embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the basic settings\n",
    "\n",
    "At first we need to initialize the Embeddings object. This object wants at least the name of our corpus file, which must be in the WPL (word-per-line) format. In this tutorial, you can use the [akkadian.zip](https://github.com/asahala/pmi-embeddings/tree/main/corpora) test corpus. In order to use it, you must first unzip it and change the file_name path below, e.g. to ```lex/akkadian.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"akk_corpus.wpl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings object can also take various other parameters summarized as follows:\n",
    "\n",
    "| Parameter | Type | What it does |\n",
    "| :- | -: | :- |\n",
    "| **chunk_size** | int | Defines how many words of the corpus are being processed at the time. Lower value decreases memory usage but increases the runtime. |\n",
    "| **window_size** | int | Defines how many preceding and following words are taken as the context. |\n",
    "| **min_count** | int | Words that have frequency lower than the set value are ignored in the co-occurrence matrix. |\n",
    "| **subsampling_rate** | float | Rate for Word2vec style subsampling. Subsampling randomly removes words that have a frequency higher than some given threshold. A typical value used is 0.00001 but in small corpora (< 10M words) subsampling is generally not very useful. |\n",
    "| **k_factor** | int | Defines the magnitude of Context Similarity Weighting, that is, a method of downsampling duplication and repetition in the corpus. Value of 0 sets it off. Useful values are typically between 1 and 3. |\n",
    "| **dynamic_window** | bool | Dynamic window gives less importance to co-occurrences if the words are far apart. |\n",
    "| **window_scaling** | bool | Window scaling compensates the co-occurrence frequencies based on the window size. It ensures that a word cannot occur with another several times within the same window. |\n",
    "| **dirty_stopwords** | bool | If set true, all stopwords are deleted without placeholders (shortens the distance between relevant words. |\n",
    "| **verbose** | bool | If set true, the script will output processing information. | \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 400000\n",
    "parameters = {\n",
    "    \"window_size\": 3,\n",
    "    \"min_count\": 1,\n",
    "    \"subsampling_rate\": None,\n",
    "    \"k_factor\": 3,\n",
    "    \"dynamic_window\": True,\n",
    "    \"window_scaling\": False,\n",
    "    \"dirty_stopwords\": False,\n",
    "    \"verbose\": True\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our all parameters set, we can initialize our Embeddings object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embs.Cooc(file_name, chunk_size, **parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to build a co-occurrence matrix from our corpus. This matrix is of size wÃ—w, where w = number of unique words in our corpus. The co-occurrence matrix contains information how many times words co-occur in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ------------------------------------------------\n",
      "> Reading akk_corpus.wpl...\n",
      "   Corpus statistics:\n",
      "      spans       6954\n",
      "      tokens      926074\n",
      "      types       5968\n",
      "      lacunae     292127\n",
      "      stopwords   229210\n",
      "      frag. rate: 0.32\n",
      "    (0.62 seconds)\n",
      "> ------------------------------------------------\n",
      "> Extracting bigrams...\n",
      "> Matrix size: 35617024 (1947 kB)\n",
      "> Non-zero elements: 243419\n",
      "    (3.11 seconds)\n",
      "> ------------------------------------------------\n",
      "> Calculating context similarities...\n",
      "    (3.23 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "embeddings.count_cooc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate a PMI matrix based on our co-occurrence matrix. PMI is an association measure that measures the statistical significance of word co-occurrences. Words that have statistically significant co-occurrences are given a score of >0 and words that seem to repulse others are given a score of <0. If the co-occurrence is statistically independent, it has a score of 0.\n",
    "\n",
    "In word embeddings the PMI values are often shifted and cut. Shifted PMI generally subtracts a small constant number from the PMI scores to get rid of borderline significant associations that are very close to being independent. The scores can be also cut, completely removing all the scores below a certain threshold. This threshold is typically set 0 (remove all repulsive co-occurrences). This is known as PPMI or Positive PMI. \n",
    "\n",
    "| Parameter | Type | What it does |\n",
    "| :- | -: | :- |\n",
    "| **threshold** | int | Defines the shift value _s_ for Shifted PMI. See _shift_type_ for more information. | \n",
    "| **shift_type** | int | Defines which formula is used for Shifted PMI. Value 0 cuts the PMI scores at _-s_. Value 1 shifts the PMI towards negative by $log_{2}$ _s_. |\n",
    "| **lambda_** | float | Adds smoothing to the co-occurrence matrix before calculating PMI as in Jungmaier et al. (2020). This method reduces PMI's bias toward rare words. Small values such as 0.0001 have been reported to work well. |\n",
    "| **alpha** | float | Context Distribution Smoothing (Levy et al. 2015). This also compensates PMI's bias toward rare words. A value of 0.75 has been reported to work well. |\n",
    "\n",
    "We can again define the parameters neatly as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_parameters = {\n",
    "    'shift_type': 0,\n",
    "    'alpha': None,\n",
    "    'lambda_': None, \n",
    "    'threshold': 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the PMI matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Calculating PMI...\n",
      "    (0.03 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "embeddings.calculate_pmi(**pmi_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a huge (Shifted) PMI matrix that is of the same size as our previous co-occurrence matrix. Most of its values are zero, because most of the words never co-occur with each in our corpus, especially if we are using a moderately small window size (e.g. 1-3), which is often recommended.\n",
    "\n",
    "The next step is to factorize the PMI matrix by using a method called Truncated [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD). This yields us a dense matrix that consists only of the values that best describe our co-occurrences in the corpus. The size of this matrix is arbitrary: it's other dimension still equals the number of unique words in our corpus, but the other dimension will be set to a fixed size, often something between 50 and 300.\n",
    "\n",
    "Lower dimensionality often results into better generalization, but this may not always be true. The best value is often found by trial-and-error and depend on the task we want to use our word embeddings for.\n",
    "\n",
    "Let's assume that we have a toy PMI matrix that would look like this:\n",
    "\n",
    "| | puppy | small | pet | wood | stone | ... |\n",
    "|--|--|--|--|--|--|--|\n",
    "|dog|5.5|1.4 |4.0 |0.02 |0.0 | ... |\n",
    "|cat|0.4|1.8 |3.9 |0.0 | 0.01| ... |\n",
    "|house|0.3 |1.5 | 0.6| 2.1| 2.5|...|\n",
    "|castle|0.0| 0.3| 0.0| 0.9| 4.0|...|\n",
    "\n",
    "The SVD matrix truncated into two dimensions could look like this:\n",
    "\n",
    "| | feature1 | feature2 |\n",
    "|--|--|--|\n",
    "|dog|1.6|0.01|\n",
    "|cat|1.4|0.01|\n",
    "|house|0.1|1.8 |\n",
    "|castle|0.0|1.9|\n",
    "\n",
    "As we can see, the vectors (rows) for cat and dog look quite similar, but not very similar to those of house and castle. For the animals, the first value is high and the second is low, while for the buildings it is the opposite.\n",
    "\n",
    "Now, we will set our dimensionality and run the matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SVD...\n",
      "> Normalizing vectors...\n",
      "    (0.76 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dimensions = 60\n",
    "embeddings.factorize(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to save our word embeddings into Word2vec compatible format. This can be done by calling the save_word_vectors() function and passing it a vector file name and our Embeddings object containing the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Filtering zero-vectors...\n",
      "> Saving 4481 non-zero vectors (1487 discarded)... \n",
      "    (0.57 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vector_file = 'akkadian.vec'\n",
    "\n",
    "embeddings.save_vectors(vector_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
